1. Update bash startup script(.bashrc) with the following, if missing.
	PATH=$PATH:"/usr/local/spark/bin"
	
2. Quick check on spark: Launch the shell with spark-shell
 
3. Build the spark code( Currently only building for local, there seems to be a mismatch between scala version being
   used for the build and the scala version the spark has been compiled with. Waiting for Sitakanta's reply.
   Will update...)

4. build the code using maven
   mvn clean
   mvn package

5. Execute the spark program(standalone local mode):
   Currently executing a simple wordcount program
   "spark-submit --class cs455.spark.startup.StartUp ./target/HW4-PC-1.0.jar local <Input HDFS path> <output HDFS path>"
   
6. Execute the spark program on yarn(yarn):
   -Login to your resource manager m/c.
   -Start resource manager. 
   Submit Job using the following command.(There are warnings , we need to probably resolve later. But I am hoping to start some basic task with it)
   "spark-submit --master yarn --class cs455.spark.startup.StartUp ./target/HW4-PC-1.0.jar yarn <Input HDFS path> <output HDFS path>"

7. Data directory
    hdfs:/Input HDFS path/employment_data